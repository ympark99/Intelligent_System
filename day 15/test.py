# 범주형 종속변수의 경우 값의 개수 비율이 중요함
# 값의 개수 비율이 중요한 이유!!(중요)
# - 머신러닝으로 예측하려는 데이터는 데이터의 비중이 작음
# - 극단적인 케이스로 악성 1%, 악성x 99%
#   (악성이 아님으로 예측만 수행해도 99%의 정확도를 가짐)
# - 데이터의 비율에서 많은 차이가 발생하는 경우
#   (오버샘플링 / 언더 샘플링))

# KNeighborsRegressor 회귀 예측을 수행할 수 있는 클래스
# 최근접 이웃을 사용하여 회귀 예측을 수행하는 경우 분류모델과의 차이점은 다수결이 아닌 평균의 값을 반환하는 점입니다.
# 최근접 이웃 알고리즘의 단점(한계점)
# fit 메소드에서 입력된 x 데이터의 범위를 벗어나면 양 끝단의 값으로만 예측을 수행함
# (학습 시 저장된 값 내에서만 예측이 가능)


# 유클리드 거리 (두 값의 차이를 제곱한 후, 제곱근 변환 진행)
# 동일 특성사이에서 차이를 계산
# - (rate - new_rate) ** 2 + (price - price) ** 2
# - (0.3 - 0.81) + (10000 - 7000) = 3000.51 (0)
# - (0.8 - 0.81) + (5000 - 7000) = 2000.51 (1)
# - (0.9 - 0.81) + (9000 - 7000) = 2500.09 (0)

# 최근접 이웃 알고리즘의 학습 및 예측방법
# - 학습 : fit 메소드에 입력된 데이터를 단순 저장
# - 예측 : fit 메소드에 의해서 저장된 데이터와
#           예측하고자 하는 신규 데이터와의 유클리드 거리를 계산하여
#           가장 인접한 n_neightbors 개수를 사용하여 이웃을 추출
#           추출된 이웃의 y을 사용하여 다수결의 과정을 수행

# 데이터의 분할
# - 회귀 분석을 위한 데이터 셋의 경우
#   y 데이터 내부의 값의 분포 비율을 유지할 필요가 없음
# - 비율이 중요한 경우에는 stratify를 사용하여 층화추출방법을 사용

# 회귀 분석
# - 머신러닝이 예측해야하는 정답의 데이터가 연속된 수치형인 경우를 의미함
# - 분류 분석의 경우 정답 데이터는 범주형[남자/여자, 탈퇴/유지 ...]
# - 선형 방정식을 활용한 머신러닝 실습

# load_  : 연습을 위한 간단한 데이터 셋
# fetch_ : 실 데이터 셋 (상대적으로 데이터의 개수가 많음)

# X 데이터를 구성하고 있는 각 특성(피처)들의 
# 스케일 범위를 반드시 확인
# - Population 컬럼에서 스케일의 차이가 발생하는 것을 확인할 수 있음

# 스케일을 동일한 범위로 수정하기 위한 전처리 방법
# - 정규화, 일반화
# - StandardScaler, MinMaxScaler

# 각 컬럼(특성, 피처)들에 대해서 산포도, 비율 등을 시각화 과정을
# 통해서 확인 및 전처리 체크를 수행


# 종속변수의 확인
# - 연속된 수치형 데이터 임을 확인
# - 회귀 분석을 위한 데이터 셋

# 설명변수 : 집값 데이터를 예측하기 위한 피처 데이터를 저장하는 변수
#           (2차원 데이터)
# 종속변수(정답) : 설명변수를 사용하여 예측하기 위한 변수
#                 (설명변수의 행수와 동일한 크기의 1차원 배열)

# 선형 방정식을 기반으로 회귀 예측을 수행할 수 있는 클래스
# - y = x1 * w1 + x2 * w2 + ..... xN * wN + b
# - LinearRegression 클래스의 학습은 X 데이터를 구성하는 
#   각 컬럼(특성, 피처) 별 최적화된 가중치와 절편의 값을
#   계산하는 과정을 수행

# 평가 (score 메소드)
# - 분류를 위한 클래스
#  : 정확도(Accuracy) : 전체 데이터 중 정답으로 맞춘 비율
# - 회귀를 위한 클래스
#  : R2 Score(결정계수) : - ~ 1 까지의 범위를 가지는 평가값

# R2(결정계수) 계산 공식
# 1 - ((실제 정답과 모델이 예측한 값의 차이의 제곱 값 합계) / 
#      (실제 정답과 실제 정답의 평균 값 차이의 제곱값 합계))

# R2(결정계수)의 값이 0 인 경우
# - 머신러닝 모델이 예측한 값이 전체 정답의 평균으로만 예측하는 경우
# - 머신러닝 모델의 학습이 부족함 (과소적합)

# R2(결정계수)의 값이 1 인 경우
# - 머신러닝 모델이 예측하는 값이 실제 정답과 완벽하게 일치하는 경우
# - 머신러닝 모델의 학습이 잘 진행됨을 확인 (과대적합의 의심...)

# R2(결정계수)의 값이 0 보다 작은 경우
# - 머신러닝 모델이 예측하는 값이 정답들의 평균조차 예측하지 못하는 경우
# - 머신러닝 모델의 학습이 부족함 (과소적합)

# - R2(결정계수)의 값은 0.7, 0.8 이상을 목표치로 설정

# 회귀분석을 위한 머신러닝 모델의 평가 함수
# - score 메소드를 사용 : R2(결정계수)

# - R2(결정계수) : 데이터에 관계없이 동일한 결과의 범위를 사용하여 모델을 평가

# - 평균절대오차 : 실제 정답과 모델이 예측한 값의 차이를 절대값으로 평균
#                 (머신러닝 모델이 예측한 값의 신뢰 범위)
# - 평균절대오차비율 : 실제 정답과 모델이 예측한 값의 비율 차이를 절대값으로 평균

# - 평균제곱오차 : 실제 정답과 모델이 예측한 값의 차이의 제곱값 평균
#                 (머신러닝/딥러닝 모델의 오차 값을 계산할 때 사용)


# 선형 모델이 학습한 가중치를 활용하여 중요도를 파악하는 방법
# - 설명변수 X를 구성하는 각 특성 별 중요도
# - 특정 컬럼(특성, 피처)에 대한 가중치의 값이 0 이라면 
#   결과에 영향을 주지않는 특성 임을 확인
# - 특정 컬럼에 대한 가중치가 다른 컬럼에 비해 상대적으로 높음
#   (회귀 분석의 경우 해당 특성의 값은 종속변수의 값을 증가시키기 
#    위해서 중요도가 높음)
# - 가중치의 절대값이 클 수록 영향력이 높은 특성임!

# LinearRegression 클래스는 학습 데이터를 예측하기 위해서
# 각각의 특성 별로 최적화된 가중치의 값을 계산하는 머신러닝 알고리즘.
# - LinearRegression이 학습한 가중치는 학습 데이터에 베스트 핏이 되는 가중치

# 머신러닝을 개발하는 이유???
# - 과거의 데이터를 통해서 새로운 데이터의 결과를 예측하기 위함
# - (생성된 가중치는 학습 데이터에 핏팅 - 완벽하게 베스트 핏팅)
# - (새로운 데이터에 잘 적합할 수 있을까?????)

# 선형 모델은 위의 문제를 해결하기 위해서 제약의 방식을 사용
# - L1 제약 (Lasso)
# - L2 제약 (Ridge)


# 선형 모델에 제약조건 (L2, L1 제약조건)을 추가한 클래스

# 선형 모델에 L2 제약 조건을 추가한 Ridge 클래스
# L2 제약 조건 : 모든 특성에 대한 가중치의 값을
# 0 주변으로 위치하도록 제어하는 제약조건
# LinearRegression 클래스는 학습 데이터에 최적화되도록
# (오차가 최소화되는 방향)
# 학습을 하기때문에 테스트 데이터에 대한 일반화 성능이 감소됩니다.
# 이러한 경우 모든 특성 데이터를 적절히 활용할 수 있도록
# L2 제약 조건을 사용할 수 있으며, L2 제약조건으로 인하여
# 모델의 일반화 성능(테스트 데이터의 성능)이 증가하게 됩니다.

# 선형 모델에 L1 제약 조건을 추가한 Lasso 클래스
# L1 제약 조건 : 모든 특성 데이터 중 특정 특성에 
# 대해서만 가중치의 값을 할당하는 제약조건
# (대다수 특성의 가중치 값은 0으로 제약)
# L1 제약 조건은 특성 데이터가 많은 데이터를 학습하는 경우 
# 빠르게 학습을 할 수 있는 장점을 가짐
# 모든 특성 데이터 중 중요도가 높은 특성을 구분할 수 있음

# Ridge, Lasso 클래스의 하이퍼 파라메터 alpha
# alpha의 값이 커질수록 제약을 크게 설정
# (alpha의 값이 커질수돌 모든 특성들의 가중치의 값은 
# 0 주변으로 위치함)
# alpha의 값이 작아질수록 제약이 약해짐
# (alpha의 값이 작아질수록 모든 특성들의 가중치의 값은 
# 0에서 멀어짐)
# alpha의 값이 작아질수록 LinearRegression 클래스와 동일해짐

# Ridge, Lasso 클래스의 하이퍼 파라메터 alpha
# alpha의 값이 커질수록 제약을 크게 설정
# (alpha의 값이 커질수돌 모든 특성들의 가중치의 값은 
# 0 주변으로 위치함)
# alpha의 값이 작아질수록 제약이 약해짐
# (alpha의 값이 작아질수록 모든 특성들의 가중치의 값은 
# 0에서 멀어짐)
# alpha의 값이 작아질수록 LinearRegression 클래스와 동일해짐

# Lasso 클래스를 사용하여 모델을 구축하면
# 대다수의 특성 가중치는 0으로 수렴(alpha 값에 따라서 조정)

# 선형 모델의 성능 향상을 위한 방법
# 1. 스케일 전처리 (정규화 / 일반화)
#  - 선형 모델은 각각의 특성 대해서 가중치를 할당하는 방식
#  - 각각의 특성의 스케일이 차이가 발생하면 가중치 적용에 어려움
# 2. 차원을 확장
#  - 선형 모델의 방정식 : y = x1 * w1 + x2 * w2 ... xN * wN + b (1차 방정식)
#  - 기본적으로 데이터 분석에 1차원의 직선을 사용하여 데이터를 예측

# 차원을 확장하여 데이터에 대한 성능을 극대화
# 1. 선형 방정식 (직선)
# - y = x * w + b

# 2. 다차원 선형 방정식 (포물선)
# - y = x*2 * w1 + x * w2 + b


# 가중치의 절대값이 클 수록 영향력이 높은 특성!

# array([[148,   0],
#        [ 42, 208]], dtype=int64)

# array([[142,   6],
#        [  8, 242]], dtype=int64)

# 머신러닝 모델의 예측    0    1  
# 실제 0인 데이터     [[141,   7]
# 실제 1인 데이터      [  5, 245]]

# 정밀도(0) : 141 / (141 + 5)
# 재현율(0) : 141 / (141 + 7)

# 앙상블 (Ensemble)
# - 다수개의 모델을 결합하나의 예측을 할 수 있는 결합 모델
# - 다수결의 원칙이 적용 (분류)
# - 평균 원칙이 적용 (회귀)

# 앙상블은 가장 좋은 평가 성적을 반환하지 않음
# - 앙상블은 하나의 모델 객체를 사용하지 않고
#   다수개의 모델 결과를 사용하여 다수결/평균을 취하므로
#   앙상블을 구성하는 많은 모델에서 가장 좋은 성적의
#   모델보다는 항상 평가가 떨어질 수 있음
# - 일반화 성능을 극대화하는 모델임
#   (테스트 데이터에 대한 성능)

# 앙상블을 구현하는 방법
# 1. 취합
# - 앙상블 구성하고 있는 각각의 모델이 독립적으로 동작
# - 각각의 모델이 독립적으로 학습하고 예측한 결과를 반환하여
#   최종적으로 취합된 결과를 다수결/평균으로 예측함
# - Voting, Bagging, RandomForest
# - 취합 기반의 앙상블 내부의 모델들은 각각 일정 수준이상의
#   예측 성능을 달성해야함
# - 학습과 예측의 속도가 빠름(병렬처리가 가능한 구조)

# 2. 부스팅
# - 앙상블을 구성하는 각각의 모델이 선형으로 결합되어
#   점진적으로 학습의 성능을 향상시켜 나가는 방법
# - 부스팅의 첫번째 모델이 예측 결과 * 1번째 모델의 가중치 + 
#   두번째 모델이 예측한 결과 * 2번째 모델의 가중치 +
#   N번째 모델이 예측한 결과 * N번째 모델의 가중치 = 결과
# - AdaBoosting, GradientBoosting, XGBoost, LightGBM
# - 부스팅 기반의 앙상블 내부 모델들은 강한 제약 조건을 설정하여
#   점진적으로 성능이 향상될 수 있도록 제어
# - 학습과 예측의 속도가 느림(순차적으로 처리해야 하므로)

# 앙상블 기반의 클래스를 로딩
# - 랜덤포레스트 : 배깅 앙상블에 결정트리를 조합한 모델이
#   주로 사용되어 하나의 클래스로 정의한 모델

# 앙상블 기반의 클래스를 로딩
# - 부스팅 : 내부의 모델들이 선형으로 결합되어
#           순차적으로 학습/예측을 수행하는 모델

# - AdaBoosting : 데이터의 관점으로 성능을 향상시켜 나가는 방법
# - GradientBoosting : 오차의 관점에서 성능을 향상시켜 나가는 방법


# 데이터 전처리
# 1. 문자열
# - 결측 데이터
# - 라벨 인코딩
# - 원핫 인코딩

# 2. 수치형
# - 결측 데이터
# - 이상치 제거(대체)
# - 스케일링

# 머신러닝을 사용하여 데이터를 분석하는 과정
# 1. 데이터 셋 로딩
# 2. 데이터의 전처리
# 3. 데이터 셋 분할
# 머신러닝 모델 생성
# 생성된 모델 사용하여 성능예측
# 교차검증 결과의 확인 평가
# 머신러닝 모델 학습
# 머신러닝 모델 평가

# ### 전처리 적용할 컬럼 식별

# - ColumnTransformer
#     - 범주형과 수치형 모두 전처리 가능
#     - 문자열, 수치형 인코더 받고 그에 맞는 인코더로 반환

# ### 교차검증

# - Training set과 validation을 여러번 나눈 뒤 모델의 학습을 검증하는 방식
# - 과적합을 막기 위해 사용
# - data set을 바꿔가며 훈련하면서 나온 평균을 정확도로 봄
# - cross_val_score 함수
#     - 교차검증 개수에 정의된 크기의 예측기 객체가 생성
#     - 각 예측기의 평가 점수가 반환됨
#     - 회귀 모델 → R2 score, 분류 모델 → 정확도 리턴

# ### K-Fold

# - k개의 fold를 만들어서 진행하는 알고리즘 → 교차검증에 사용
# - 총 data set 개수가 적은 data set에 대하여 정확도를 향상시킬 수 있음
# - K-Fold 객체를 cross_val_score 함수의 cv 매개변수로 사용할 수 있음
# - **보통 회귀 모델에 사용되며, 데이터가 독립적이고 동일한 분포를 가진 경우에 사용**
# - 단점
#     - 시간 소요가 크다

# ### SimpleImputer

# - data set의 missing value를 특정한 값으로 채우는 기능

# ### K-Fold → shuffle 매개변수

# - KFold 클래스의 객체를 생성할 때, shuffle 매개변수를 지정하지 않는 경우 데이터를 순차적으로 분할

# - 따라서 라벨이 정렬된 데이터에서는 잘못된 분석 결과가 나올 수 있음
# - shuffle True → 정답 데이터의 비율을 균등하게 포함하는 폴드들을 생성

# ### 데이터 전처리

# 1. 문자열
#     - 결측 data
#     - 라벨 인코딩
#     - 원핫 인코딩
# 2. 수치형
#     - 결측 데이터
#         - 이상치 제거
#     - 스케일링
# - 수치는 평균, 문자열은 최빈도로 대치 많이함
# - default는 mean(평균)
# - median(중앙값)
# - most_frequent(최빈값)
# - constant(지정값)

# ### 하이퍼 파라미터

# - **모델링할때 사용자가 직접 세팅해주는 값**

# ## 12주차

# - x_train으로 학습, x_test로 테스트 → 내신 잘하나 수능 망칠수도 있음, k-fold로 처리

# ### Grid Search

# - 모델에게 가장 적합한 **하이퍼 파라메터**를 찾는 것
#     - 하이퍼 파라메터 : 모델을 생성할 때, 사용자가 직접 설정하는 변수
#     - 검증 데이터 분할 방법에 따라서 값이 달라짐
# - 교차 검증에 사용할 기본 머신러닝 모델
#     - 테스트할 하이퍼 파라메터는 설정에서 제외
#     - 공통 하이퍼 파레미터 정보만 사용하여 모델을 정의
#     - 학습 X
# - 스케일의 편차가 존재 → 정규화 처리 필요

# ### Grid Search 장단점

# - 장점
#     - 내가 원하는 범위를 정확하게 비교 분석 가능
# - 단점
#     - 모든 경우의 수를 넣었기 때문에 시간이 많이 걸림

# ### pipeline

# - 스케일링과 다양한 학습들을 한번에 해주는 방법

# ### classification_report

# - 분류 모델 평가 지표 한번에 볼 수 있음 (정확도, 정밀도 등…)

# ### pipeline 데이터의 전처리 과정

# - 라벨 인코딩, 특성 데이터의 스케일 조정 등의 작업을 수행
# - 사이킷 런의 변환기 클래스를 활용
# - fit 메소드는 반드시 학습(train) 데이터에 대해서만 적용
# - transform 메소드를 사용하여 학습 및 테스트 데이터의 변환을 수행

# ### 교차 검증에 사용할 기본 머신러닝 객체

# - 테스트할 하이퍼 파라메터는 설정에서 제외
# - 공통 하이퍼 파라메터 정보만 사용하여 모델 정의
# - 학습 X

# ### 일반적인 머신러닝 단계

# - 데이터 전처리 단계의 추가
# - 하이퍼 파라메터 검색 단계의 추가
# - 파이프 라인을 사용한 데이터의 전처리 과정 및 머신러닝 모델의 학습 과정 자동화(전처리와 학습의 연결)
# - 파이프 라인을 사용한 하이퍼 파라메터를 검색(올바른 방식의 교차 검증을 수행)

# ### 파이프 라인 실행 과정 및 특징

# 1. fit 메소드가 호출되는 경우
# 2. score/predict 메소드가 호출되는 경우
# - 파이프라인의 `fit()` 메서드를 호출하면 모든 변환기의 `fit_transform()`메서드를 순서대로 호출하면서 **한 단계의 출력을 다음 단계의 입력으로 전달**
#     - predict()면 멈춤
# - 마지막 단계에서는 `fit()`메서드만 호출합니다.
# - 파이프 라인의 마지막 객체를 제외한 나머지 객체들은 transform, fit_transform 메소드를 제공하는 변환기만 허용
# - 파이프 라인의 마지막 객체는 predict 메소드를 제공하는 예측기 객체가 될 수 있음
# - GridSearchCV 클래스의 생성자 매개변수로 파이프 라인 객체가 사용될 수 있음
# - 재현성 높음

# ## 13주차

# ### 비지도 학습

# - 지도학습과 다르게 **종속변수(정답데이터)가 제공되지 않는 데이터에 대한 학습을 처리**하는 기법
# - 비지도학습의 결과는 주관적인 판단으로 처리
# - 비지도학습의 대표적인 종류 → 클러스터링
# - 지도학습
#     - 정답이 있는 데이터를 활용해 데이터를 학습시키는 것
#     - 최근접 이웃(k-Nearest Neighbors)
#     - 선형 회귀(Linear Regression)
#     - 로지스틱 회귀(Logistic Regression)
#     - 서포트 벡터 머신(SVM, Support Vector Machine)
#     - 결정 트리(Decision Tree)와 랜덤 포레스트(Random Forest)
# - 비지도학습
#     - **K-means 알고리즘** : 평균을 활용하여 군집을 묶는 알고리즘 → 최적의 위치로 움직임
#         - intertia : 클러스터 중심과 클러스터에 속한 샘플 사이의 거리를 잴 수 있는데, 이 거리의 제곱 합
#         - 장점 : 변경에 용이

# ### 비지도 학습의 카테고리

# - **차원 축소**
#     - 데이터에 포함된 특성 중 유의미한 값을 추출
#     - 데이터에 포함된 특성을 투영하여 대표하는 값을 반환
# - **군집 분석**
#     - 데이터의 유사성을 비교하여 동일한 특성으로 구성된 데이터들을 하나의 군집으로 처리하는 기법

# ### 비지도 학습 시 유의사항

# - 비지도 학습의 결과는 100% 신뢰할 수 없음
# - 매번 실행될 때마다 결과는 변경될 수 있음
# - 평균오차를 통해 최적의 군집의 개수를 검색할 수 있음

# ### 엘보우 기법

# - 클러스터 개수를 늘렸을 때 무게중심간의 평균 거리가 더 이상 많이 감소하지 않는 경우의 K를 선택하는 방법.
# - **개수가 늘 때마다 평균값이 급격히 감소하는데 적절한 K가 발견되면 매우 천천히 감소**한다.
# - 그래프 상에서 이 부분이 팔꿈치랑 닮아서 엘보우 기법이라고 한다.
# - 참고로, 클러스터 개수가 적으면 centroid 간의 거리가 매우 커지며, 적절한 개수이면 거리가 점점 짧아진다. 개수가 많으면 평균 거리가 매우 조금씩 줄어든다.

# ### 병합군집 - AgglomerativeClustering

# - 다수개의 소규모 군집을 생성 (랜덤하게)
# - 다수개의 소규모 군집을 취합해 하나로 병합 (**인접한** 위치의 군집사이에서 발생)
# - 원하는 개수의 군집으로 최적 처리를 완료

# ### fit_predict 메소드의 동작

# - n_clusters에 정의된 개수만큼 소규모의 군집들을 계속해서 병합한 후 정의된 개수에 도달하면 해당 정보를 반환

# ### DBSCAN

# - **데이터간의 밀도를 이용하여 군집의 형성 여부를 결정하는 알고리즘**
# - DBSCAN은 군집의 크기가 자동으로 결정

# ### 앙상블

# - 다수개의 머신러닝 모델의 예측 값을 취합하여 평균, 다수결의 원칙으로 예측하는 모델
# - 앙상블을 사용하는 이유→일반화의 성능을 극대화하기 위해서 (예측 성능의 분산을 감소시킬 수 있으므로)

# ### 스태킹

# - 다수개의 머신러닝 모델이 예측한 값을 학습하여 결과를 반환하는 방법
# - **개별 알고리즘의 예측한 데이터를 기반으로 다시 예측을 수행하는 방법**
# - 머신러닝 알고리즘에 의해서 원본 학습데이터가 하나의 결과 값으로 압축되어 머신러닝의 개수만큼의 데이터로 생성되고 **해당 데이터를 학습하여 예측하는 방식**
    
#     → 즉 한 번 더 학습시키는 느낌
    
# - 장점 : 단일 모델로 했을때 보다 성능이 확연히 향상된다.
# - 단점 : **과적합 (overfitting)**


# -*- coding: utf-8 -*-

# 스태킹 모델의 구축
# - 앙상블 : 다수개의 머신러닝 모델의 예측 값을 취합하여
#   평균, 다수결의 원칙으로 예측하는 모델
# - 앙상블을 사용하는 이유???
#   일반화의 성능을 극대화하기 위해서
#   (예측 성능의 분산을 감소시킬 수 있으므로)

# - 다수개의 머신러닝 모델이 예측 한 값을 학습하여
#   결과를 반환하는 방법 -> 스태킹
# - 머신러닝 알고리즘에 의해서 원본 학습데이터가
#   하나의 결과 값으로 압축되어 머신러닝의 개수만큰의
#   데이터로 생성되고 해당 데이터를 학습하여
#   예측하는 방식

import pandas as pd
pd.options.display.max_columns=100
from sklearn.datasets import load_breast_cancer

data = load_breast_cancer()

X = pd.DataFrame(data.data, 
                 columns=data.feature_names)
y = pd.Series(data.target, name='target')

print(X.head())
print(X.info())
print(X.describe())

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.3,
    stratify=y,
    random_state=1)


from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler().fit(X_train)

X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)

from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier

clf_lr = LogisticRegression(random_state=0).fit(X_train_scaled, y_train)
clf_kn = KNeighborsClassifier().fit(X_train_scaled, y_train)
clf_dt = DecisionTreeClassifier(random_state=0).fit(X_train_scaled, y_train)

v_score = clf_lr.score(X_train_scaled, y_train)
print(f'학습 clf_lr : {v_score}')
v_score = clf_kn.score(X_train_scaled, y_train)
print(f'학습 clf_kn : {v_score}')
v_score = clf_dt.score(X_train_scaled, y_train)
print(f'학습 clf_dt : {v_score}')

v_score = clf_lr.score(X_test_scaled, y_test)
print(f'테스트 clf_lr : {v_score}')
v_score = clf_kn.score(X_test_scaled, y_test)
print(f'테스트 clf_kn : {v_score}')
v_score = clf_dt.score(X_test_scaled, y_test)
print(f'테스트 clf_dt : {v_score}')

# 스태킹 구현
# 1. 앙상블을 구현하고 있는 각 머신러닝
# 모델들의 예측 결과를 취합
pred_lr = clf_lr.predict(X_train_scaled)
pred_kn = clf_kn.predict(X_train_scaled)
pred_dt = clf_dt.predict(X_train_scaled)

import numpy as np
pred_stack = np.array([pred_lr, pred_kn, pred_dt])
# print(pred_stack)

# print(y_train.shape)
# print(pred_stack.shape)

pred_stack = pred_stack.T

# print(y_train.shape)
# print(pred_stack.shape)

# print(pred_stack[:5])

from sklearn.ensemble import RandomForestClassifier
final_model = RandomForestClassifier(n_estimators=100,
                                     max_depth=None,
                                     max_samples=0.5,
                                     max_features=0.3,
                                     random_state=1).fit(pred_stack, y_train)
score = final_model.score(pred_stack, y_train)
print(f'학습 final_model : {score}')

# 평가
# - 테스트 데이터에 대한 각 모델들의 예측 값을 취합
pred_lr = clf_lr.predict(X_test_scaled)
pred_kn = clf_kn.predict(X_test_scaled)
pred_dt = clf_dt.predict(X_test_scaled)

pred_stack = np.array([pred_lr, pred_kn, pred_dt]).T

score = final_model.score(pred_stack, y_test)
print(f'테스트 final_model : {score}')




